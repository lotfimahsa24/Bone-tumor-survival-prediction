# ============================================================
# ðŸ§  SAINT-style Tabular Transformer (Improved)
# Train / Val / Test + Early Stopping + Warmup Cosine Scheduler
# Dataset: Bone Tumor Dataset.csv
# ============================================================

!pip install -q torch torchvision torchaudio

from google.colab import files
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report
)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ------------------------------------------------------------
# 1) Upload & load dataset
# ------------------------------------------------------------
print("ðŸ“‚ Please upload your 'Bone Tumor Dataset.csv' file")
uploaded = files.upload()
filename = list(uploaded.keys())[0]

df = pd.read_csv(filename)
print("Loaded:", filename)
print("Shape:", df.shape)
print(df.head(3))

# ------------------------------------------------------------
# 2) Build target: Dead (D) vs Alive (NED + AWD)
# ------------------------------------------------------------
if "Patient ID" in df.columns:
    df = df.drop(columns=["Patient ID"])

status_col = "Status (NED, AWD, D)"
if status_col not in df.columns:
    raise KeyError(f"Column '{status_col}' not found. Available columns: {df.columns.tolist()}")

df["target"] = (df[status_col] == "D").astype(int)

X = df.drop(columns=[status_col, "target"])
y = df["target"].values

print("\nClass distribution (0=Alive, 1=Dead):")
print(df["target"].value_counts())

# ------------------------------------------------------------
# 3) Split into Train (60%) / Val (20%) / Test (20%)
# ------------------------------------------------------------
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42
)  # 0.25 * 0.8 = 0.2

print("\nTrain size:", X_train.shape,
      "| Val size:", X_val.shape,
      "| Test size:", X_test.shape)

# ------------------------------------------------------------
# 4) Categorical & numeric columns + preprocessing
# ------------------------------------------------------------
categorical_cols = [c for c in X.columns if X[c].dtype == "object"]
numeric_cols = [c for c in X.columns if c not in categorical_cols]

print("\nCategorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)

X_train_t = X_train.copy()
X_val_t = X_val.copy()
X_test_t = X_test.copy()

# Label-encode categoricals based on TRAIN
cat_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X_train_t[col] = le.fit_transform(X_train_t[col].astype(str))
    X_val_t[col] = le.transform(X_val_t[col].astype(str))
    X_test_t[col] = le.transform(X_test_t[col].astype(str))
    cat_encoders[col] = le

# Scale numerics based on TRAIN
scaler = StandardScaler()
if numeric_cols:
    X_train_t[numeric_cols] = scaler.fit_transform(X_train_t[numeric_cols].astype(float))
    X_val_t[numeric_cols] = scaler.transform(X_val_t[numeric_cols].astype(float))
    X_test_t[numeric_cols] = scaler.transform(X_test_t[numeric_cols].astype(float))

X_train_t = X_train_t.reset_index(drop=True)
X_val_t = X_val_t.reset_index(drop=True)
X_test_t = X_test_t.reset_index(drop=True)

cat_train = X_train_t[categorical_cols].values.astype(np.int64) if categorical_cols else None
cat_val = X_val_t[categorical_cols].values.astype(np.int64) if categorical_cols else None
cat_test = X_test_t[categorical_cols].values.astype(np.int64) if categorical_cols else None

num_train = X_train_t[numeric_cols].values.astype(np.float32) if numeric_cols else None
num_val = X_val_t[numeric_cols].values.astype(np.float32) if numeric_cols else None
num_test = X_test_t[numeric_cols].values.astype(np.float32) if numeric_cols else None

# ------------------------------------------------------------
# 5) Dataset & DataLoaders
# ------------------------------------------------------------
class TabularDataset(Dataset):
    def __init__(self, cat_data, num_data, y):
        self.cat_data = cat_data
        self.num_data = num_data
        self.y = y.astype(np.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        cat = self.cat_data[idx] if self.cat_data is not None else None
        num = self.num_data[idx] if self.num_data is not None else None
        label = self.y[idx]
        return cat, num, label

train_ds = TabularDataset(cat_train, num_train, y_train)
val_ds = TabularDataset(cat_val, num_val, y_val)
test_ds = TabularDataset(cat_test, num_test, y_test)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)
test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)

# ------------------------------------------------------------
# 6) SAINT-style Transformer model
# ------------------------------------------------------------
class SAINTTabular(nn.Module):
    def __init__(self, cat_cardinalities, num_features,
                 d_model=64, n_heads=4, n_layers=2, dropout=0.3):
        super().__init__()
        self.has_cat = len(cat_cardinalities) > 0
        self.has_num = num_features > 0

        self.d_model = d_model

        if self.has_cat:
            self.cat_embeddings = nn.ModuleList(
                [nn.Embedding(card, d_model) for card in cat_cardinalities]
            )
        if self.has_num:
            self.num_linear = nn.Linear(num_features, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            activation="gelu",
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        self.norm = nn.LayerNorm(d_model)

        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, 1)
        )

    def forward(self, cat_data, num_data):
        tokens = []

        if self.has_cat and cat_data is not None:
            cat_embs = []
            for i, emb in enumerate(self.cat_embeddings):
                cat_embs.append(emb(cat_data[:, i]))  # (B, d_model)
            cat_embs = torch.stack(cat_embs, dim=1)  # (B, n_cat, d_model)
            tokens.append(cat_embs)

        if self.has_num and num_data is not None:
            num_emb = self.num_linear(num_data)      # (B, d_model)
            num_emb = num_emb.unsqueeze(1)          # (B, 1, d_model)
            tokens.append(num_emb)

        x = torch.cat(tokens, dim=1)                # (B, seq_len, d_model)
        x = self.transformer(x)                     # (B, seq_len, d_model)
        x = x.mean(dim=1)                           # mean pooling
        x = self.norm(x)
        logits = self.fc(x).squeeze(-1)
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("\nUsing device:", device)

cat_cardinalities = []
if categorical_cols:
    for col in categorical_cols:
        card = len(cat_encoders[col].classes_)
        cat_cardinalities.append(card)

num_features = len(numeric_cols)

model = SAINTTabular(
    cat_cardinalities=cat_cardinalities,
    num_features=num_features,
    d_model=64,
    n_heads=4,
    n_layers=2,
    dropout=0.3,
).to(device)

# ------------------------------------------------------------
# 7) Loss, optimizer, scheduler (warmup + cosine), early stopping
# ------------------------------------------------------------
# class imbalance handling: pos_weight for "Dead" class
n_pos = (y_train == 1).sum()
n_neg = (y_train == 0).sum()
pos_weight = torch.tensor(n_neg / max(n_pos, 1), dtype=torch.float32, device=device)
print(f"\npos_weight for BCEWithLogitsLoss: {pos_weight.item():.3f}")

criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)

class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):
    """
    Cosine annealing with linear warmup.
    step() should be called once per optimizer step.
    """
    def __init__(self, optimizer, warmup_steps, max_steps, last_epoch=-1):
        self.warmup_steps = max(1, warmup_steps)
        self.max_steps = max_steps
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        step = self.last_epoch + 1

        if step <= self.warmup_steps:
            # linear warmup
            return [base_lr * step / self.warmup_steps for base_lr in self.base_lrs]

        if step >= self.max_steps:
            # after max_steps, keep lr at a small value
            return [base_lr * 0.1 for base_lr in self.base_lrs]

        # cosine decay
        progress = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
        return [
            base_lr * 0.5 * (1 + math.cos(math.pi * progress))
            for base_lr in self.base_lrs
        ]

EPOCHS = 80
total_steps = EPOCHS * len(train_loader)
warmup_steps = int(0.1 * total_steps)  # 10% warmup

scheduler = CosineWarmupScheduler(
    optimizer,
    warmup_steps=warmup_steps,
    max_steps=total_steps
)

def get_auc_safe(y_true, y_proba):
    try:
        return roc_auc_score(y_true, y_proba)
    except ValueError:
        return np.nan

# ------------------------------------------------------------
# 8) Train / Eval functions
# ------------------------------------------------------------
def train_epoch(model, loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0.0
    all_probs = []
    all_labels = []

    for cat_batch, num_batch, y_batch in loader:
        if cat_batch is not None:
            cat_batch = cat_batch.to(device)
        if num_batch is not None:
            num_batch = num_batch.to(device)
        y_batch = y_batch.to(device)

        optimizer.zero_grad()
        logits = model(cat_batch, num_batch)
        loss = criterion(logits, y_batch)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item() * y_batch.size(0)
        probs = torch.sigmoid(logits)
        all_probs.append(probs.detach().cpu().numpy())
        all_labels.append(y_batch.detach().cpu().numpy())

    avg_loss = total_loss / len(loader.dataset)
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    return avg_loss, all_probs, all_labels

def eval_epoch(model, loader, device):
    model.eval()
    total_loss = 0.0
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for cat_batch, num_batch, y_batch in loader:
            if cat_batch is not None:
                cat_batch = cat_batch.to(device)
            if num_batch is not None:
                num_batch = num_batch.to(device)
            y_batch = y_batch.to(device)

            logits = model(cat_batch, num_batch)
            loss = criterion(logits, y_batch)

            total_loss += loss.item() * y_batch.size(0)
            probs = torch.sigmoid(logits)
            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())

    avg_loss = total_loss / len(loader.dataset)
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    return avg_loss, all_probs, all_labels

# ------------------------------------------------------------
# 9) Training loop with early stopping on Val AUC
# ------------------------------------------------------------
best_val_auc = -np.inf
best_state = None
patience = 15
no_improve = 0

print("\nðŸš€ Training improved SAINT-style Transformer (with warmup + early stopping)...")
for epoch in range(1, EPOCHS + 1):
    train_loss, train_proba, train_labels = train_epoch(model, train_loader, optimizer, scheduler, device)
    val_loss, val_proba, val_labels = eval_epoch(model, val_loader, device)

    train_auc = get_auc_safe(train_labels, train_proba)
    val_auc = get_auc_safe(val_labels, val_proba)

    if epoch == 1 or epoch % 10 == 0:
        print(
            f"Epoch {epoch:03d}/{EPOCHS} | "
            f"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | "
            f"Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}"
        )

    if val_auc > best_val_auc:
        best_val_auc = val_auc
        best_state = model.state_dict()
        no_improve = 0
    else:
        no_improve += 1
        if no_improve >= patience:
            print(f"â¹ Early stopping at epoch {epoch} (no improvement in Val AUC for {patience} epochs).")
            break

print(f"\nBest Val AUC: {best_val_auc:.4f}")
if best_state is not None:
    model.load_state_dict(best_state)

# ------------------------------------------------------------
# 10) Final evaluation on Train / Val / Test
# ------------------------------------------------------------
def eval_split(loader, y_true_full, split_name):
    loss, proba, _ = eval_epoch(model, loader, device)
    proba = proba.reshape(-1)
    y_true = y_true_full
    y_pred = (proba >= 0.5).astype(int)

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    auc = get_auc_safe(y_true, proba)

    print("\n==============================")
    print(f" SAINT-style Transformer on {split_name}")
    print("==============================")
    print(f"Accuracy     : {acc:.4f}")
    print(f"Precision    : {prec:.4f}")
    print(f"Recall       : {rec:.4f}")
    print(f"F1-score     : {f1:.4f}")
    print(f"AUC          : {auc:.4f}")
    print("\nClassification report:")
    print(classification_report(y_true, y_pred, target_names=["Alive", "Dead"]))

    return y_true, y_pred, proba

y_train_true, y_train_pred, train_proba_final = eval_split(train_loader, y_train, "TRAIN")
y_val_true, y_val_pred, val_proba_final = eval_split(val_loader, y_val, "VALIDATION")
y_test_true, y_test_pred, test_proba_final = eval_split(test_loader, y_test, "TEST")

# ------------------------------------------------------------
# 11) Confusion matrix & ROC for TEST
# ------------------------------------------------------------
cm = confusion_matrix(y_test_true, y_test_pred)
tn, fp, fn, tp = cm.ravel()
spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan
print(f"\nSpecificity (Test): {spec:.4f}")

plt.figure(figsize=(4, 4))
sns.heatmap(
    cm, annot=True, fmt="d", cmap="Blues",
    xticklabels=["Pred Alive", "Pred Dead"],
    yticklabels=["True Alive", "True Dead"]
)
plt.title("Confusion Matrix - SAINT (Test)")
plt.tight_layout()
plt.show()

fpr, tpr, _ = roc_curve(y_test_true, test_proba_final)
test_auc = roc_auc_score(y_test_true, test_proba_final)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"SAINT Test (AUC = {test_auc:.4f})")
plt.plot([0, 1], [0, 1], "--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - SAINT (Test)")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()
