import os
import math
import argparse
import random

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix
)

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


# ============================================================
# Utils
# ============================================================
def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def get_auc_safe(y_true, y_proba):
    try:
        return roc_auc_score(y_true, y_proba)
    except ValueError:
        return np.nan


# ============================================================
# Data loading & preprocessing
# ============================================================
def load_and_preprocess(csv_path: str):
    df = pd.read_csv(csv_path)
    print(f"Loaded: {csv_path}")
    print("Shape:", df.shape)
    print(df.head(3))

    if "Patient ID" in df.columns:
        df = df.drop(columns=["Patient ID"])

    status_col = "Status (NED, AWD, D)"
    if status_col not in df.columns:
        raise KeyError(
            f"Column '{status_col}' not found. Available columns: {df.columns.tolist()}"
        )

    df["target"] = (df[status_col] == "D").astype(int)

    X = df.drop(columns=[status_col, "target"])
    y = df["target"].values

    print("\nClass distribution (0=Alive, 1=Dead):")
    print(df["target"].value_counts())

    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_trainval, y_trainval, test_size=0.25,
        stratify=y_trainval, random_state=42
    )

    print(
        f"\nTrain size: {X_train.shape} | "
        f"Val size: {X_val.shape} | "
        f"Test size: {X_test.shape}"
    )

    categorical_cols = [c for c in X.columns if X[c].dtype == "object"]
    numeric_cols = [c for c in X.columns if c not in categorical_cols]

    print("\nCategorical columns:", categorical_cols)
    print("Numeric columns:", numeric_cols)

    X_train_t = X_train.copy()
    X_val_t = X_val.copy()
    X_test_t = X_test.copy()

    cat_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        X_train_t[col] = le.fit_transform(X_train_t[col].astype(str))
        X_val_t[col] = le.transform(X_val_t[col].astype(str))
        X_test_t[col] = le.transform(X_test_t[col].astype(str))
        cat_encoders[col] = le

    scaler = StandardScaler()
    if numeric_cols:
        X_train_t[numeric_cols] = scaler.fit_transform(
            X_train_t[numeric_cols].astype(float)
        )
        X_val_t[numeric_cols] = scaler.transform(
            X_val_t[numeric_cols].astype(float)
        )
        X_test_t[numeric_cols] = scaler.transform(
            X_test_t[numeric_cols].astype(float)
        )

    X_train_proc = X_train_t.reset_index(drop=True)
    X_val_proc = X_val_t.reset_index(drop=True)
    X_test_proc = X_test_t.reset_index(drop=True)

    cat_train = (
        X_train_proc[categorical_cols].values.astype(np.int64)
        if categorical_cols else None
    )
    cat_val = (
        X_val_proc[categorical_cols].values.astype(np.int64)
        if categorical_cols else None
    )
    cat_test = (
        X_test_proc[categorical_cols].values.astype(np.int64)
        if categorical_cols else None
    )

    num_train = (
        X_train_proc[numeric_cols].values.astype(np.float32)
        if numeric_cols else None
    )
    num_val = (
        X_val_proc[numeric_cols].values.astype(np.float32)
        if numeric_cols else None
    )
    num_test = (
        X_test_proc[numeric_cols].values.astype(np.float32)
        if numeric_cols else None
    )

    return (
        X_train_proc, X_val_proc, X_test_proc,
        y_train, y_val, y_test,
        categorical_cols, numeric_cols,
        cat_train, cat_val, cat_test,
        num_train, num_val, num_test
    )


# ============================================================
# Tree-based models
# ============================================================
def train_xgboost(X_train, y_train, X_test, y_test, scale_pos_weight):
    model = xgb.XGBClassifier(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="binary:logistic",
        eval_metric="logloss",
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        n_jobs=-1,
        tree_method="hist"
    )
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_test)[:, 1]
    return model, proba


def train_lightgbm(X_train, y_train, X_test, y_test, scale_pos_weight):
    model = lgb.LGBMClassifier(
        n_estimators=400,
        learning_rate=0.05,
        num_leaves=31,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="binary",
        class_weight={0: 1.0, 1: float(scale_pos_weight)},
        random_state=42
    )
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_test)[:, 1]
    return model, proba


def train_catboost(X_train, y_train, X_test, y_test, scale_pos_weight):
    model = CatBoostClassifier(
        iterations=500,
        depth=6,
        learning_rate=0.05,
        loss_function="Logloss",
        eval_metric="AUC",
        class_weights=[1.0, float(scale_pos_weight)],
        random_state=42,
        verbose=False
    )
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_test)[:, 1]
    return model, proba


# ============================================================
# SAINT-style Transformer
# ============================================================
class TabularDataset(Dataset):
    def __init__(self, cat_data, num_data, y):
        self.cat_data = cat_data
        self.num_data = num_data
        self.y = y.astype(np.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        cat = self.cat_data[idx] if self.cat_data is not None else None
        num = self.num_data[idx] if self.num_data is not None else None
        label = self.y[idx]
        return cat, num, label


class SAINTTabular(nn.Module):
    def __init__(self, cat_cardinalities, num_features,
                 d_model=64, n_heads=4, n_layers=2, dropout=0.3):
        super().__init__()
        self.has_cat = len(cat_cardinalities) > 0
        self.has_num = num_features > 0
        self.d_model = d_model

        if self.has_cat:
            self.cat_embeddings = nn.ModuleList(
                [nn.Embedding(card, d_model) for card in cat_cardinalities]
            )
        if self.has_num:
            self.num_linear = nn.Linear(num_features, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True,
            activation="gelu"
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, num_layers=n_layers
        )
        self.norm = nn.LayerNorm(d_model)
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, 1)
        )

    def forward(self, cat_data, num_data):
        tokens = []

        if self.has_cat and cat_data is not None:
            cat_embs = []
            for i, emb in enumerate(self.cat_embeddings):
                cat_embs.append(emb(cat_data[:, i]))
            cat_embs = torch.stack(cat_embs, dim=1)
            tokens.append(cat_embs)

        if self.has_num and num_data is not None:
            num_emb = self.num_linear(num_data)
            num_emb = num_emb.unsqueeze(1)
            tokens.append(num_emb)

        x = torch.cat(tokens, dim=1)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.norm(x)
        logits = self.fc(x).squeeze(-1)
        return logits


class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):
    def __init__(self, optimizer, warmup_steps, max_steps, last_epoch=-1):
        self.warmup_steps = max(1, warmup_steps)
        self.max_steps = max_steps
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        step = self.last_epoch + 1
        if step <= self.warmup_steps:
            return [base_lr * step / self.warmup_steps for base_lr in self.base_lrs]
        if step >= self.max_steps:
            return [base_lr * 0.1 for base_lr in self.base_lrs]
        progress = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
        return [
            base_lr * 0.5 * (1 + math.cos(math.pi * progress))
            for base_lr in self.base_lrs
        ]


def train_epoch_saint(model, loader, optimizer, scheduler, criterion, device):
    model.train()
    total_loss = 0.0
    all_probs, all_labels = [], []

    for cat_batch, num_batch, y_batch in loader:
        if cat_batch is not None:
            cat_batch = cat_batch.to(device)
        if num_batch is not None:
            num_batch = num_batch.to(device)
        y_batch = y_batch.to(device)

        optimizer.zero_grad()
        logits = model(cat_batch, num_batch)
        loss = criterion(logits, y_batch)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item() * y_batch.size(0)
        probs = torch.sigmoid(logits)
        all_probs.append(probs.detach().cpu().numpy())
        all_labels.append(y_batch.detach().cpu().numpy())

    avg_loss = total_loss / len(loader.dataset)
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    return avg_loss, all_probs, all_labels


def eval_epoch_saint(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    all_probs, all_labels = [], []

    with torch.no_grad():
        for cat_batch, num_batch, y_batch in loader:
            if cat_batch is not None:
                cat_batch = cat_batch.to(device)
            if num_batch is not None:
                num_batch = num_batch.to(device)
            y_batch = y_batch.to(device)

            logits = model(cat_batch, num_batch)
            loss = criterion(logits, y_batch)

            total_loss += loss.item() * y_batch.size(0)
            probs = torch.sigmoid(logits)
            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())

    avg_loss = total_loss / len(loader.dataset)
    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)
    return avg_loss, all_probs, all_labels


def train_saint(
    cat_train, num_train, y_train,
    cat_val, num_val, y_val,
    cat_test, num_test, y_test,
    cat_cardinalities, num_features,
    device, seed=42
):
    train_ds = TabularDataset(cat_train, num_train, y_train)
    val_ds = TabularDataset(cat_val, num_val, y_val)
    test_ds = TabularDataset(cat_test, num_test, y_test)

    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)

    model = SAINTTabular(
        cat_cardinalities=cat_cardinalities,
        num_features=num_features,
        d_model=64,
        n_heads=4,
        n_layers=2,
        dropout=0.3
    ).to(device)

    n_pos = (y_train == 1).sum()
    n_neg = (y_train == 0).sum()
    pos_weight = torch.tensor(
        n_neg / max(n_pos, 1),
        dtype=torch.float32,
        device=device
    )
    print(f"\npos_weight for BCEWithLogitsLoss: {pos_weight.item():.3f}")

    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)

    EPOCHS = 60
    total_steps = EPOCHS * len(train_loader)
    warmup_steps = int(0.1 * total_steps)

    scheduler = CosineWarmupScheduler(
        optimizer, warmup_steps=warmup_steps, max_steps=total_steps
    )

    best_val_auc = -np.inf
    best_state = None
    patience = 12
    no_improve = 0

    print("\nTraining SAINT-B Transformer (with warmup + early stopping)...")
    for epoch in range(1, EPOCHS + 1):
        train_loss, train_proba, train_labels = train_epoch_saint(
            model, train_loader, optimizer, scheduler, criterion, device
        )
        val_loss, val_proba, val_labels = eval_epoch_saint(
            model, val_loader, criterion, device
        )

        train_auc = get_auc_safe(train_labels, train_proba)
        val_auc = get_auc_safe(val_labels, val_proba)

        if epoch == 1 or epoch % 10 == 0:
            print(
                f"Epoch {epoch:03d}/{EPOCHS} | "
                f"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | "
                f"Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}"
            )

        if val_auc > best_val_auc:
            best_val_auc = val_auc
            best_state = model.state_dict()
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(
                    f"Early stopping at epoch {epoch} "
                    f"(no improvement in Val AUC for {patience} epochs)."
                )
                break

    print(f"\nBest SAINT-B Val AUC: {best_val_auc:.4f}")
    if best_state is not None:
        model.load_state_dict(best_state)

    _, test_proba, _ = eval_epoch_saint(model, test_loader, criterion, device)
    return model, test_proba.reshape(-1)


# ============================================================
# Evaluation helpers
# ============================================================
def evaluate_probs(y_true, proba, threshold=0.5):
    y_pred = (proba >= threshold).astype(int)
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    auc = get_auc_safe(y_true, proba)
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan
    return {
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "auc": auc,
        "specificity": spec,
        "cm": cm
    }


# ============================================================
# Main
# ============================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--csv_path",
        type=str,
        default="Bone Tumor Dataset.csv",
        help="Path to Bone Tumor Dataset CSV"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs",
        help="Directory to save results"
    )
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    seed_everything(42)

    (
        X_train_proc, X_val_proc, X_test_proc,
        y_train, y_val, y_test,
        categorical_cols, numeric_cols,
        cat_train, cat_val, cat_test,
        num_train, num_val, num_test
    ) = load_and_preprocess(args.csv_path)

    n_pos = (y_train == 1).sum()
    n_neg = (y_train == 0).sum()
    scale_pos_weight = n_neg / max(n_pos, 1)
    print(f"\nscale_pos_weight for tree models: {scale_pos_weight:.3f}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("\nUsing device:", device)

    results = {}
    roc_data = {}

    # XGBoost
    xgb_model, xgb_proba = train_xgboost(
        X_train_proc, y_train, X_test_proc, y_test, scale_pos_weight
    )
    res_xgb = evaluate_probs(y_test, xgb_proba, threshold=0.5)
    results["XGBoost (base)"] = res_xgb
    fpr, tpr, _ = roc_curve(y_test, xgb_proba)
    roc_data["XGBoost (base)"] = (fpr, tpr)

    # LightGBM
    lgb_model, lgb_proba = train_lightgbm(
        X_train_proc, y_train, X_test_proc, y_test, scale_pos_weight
    )
    res_lgb = evaluate_probs(y_test, lgb_proba, threshold=0.5)
    results["LightGBM (base)"] = res_lgb
    fpr, tpr, _ = roc_curve(y_test, lgb_proba)
    roc_data["LightGBM (base)"] = (fpr, tpr)

    # CatBoost
    cb_model, cb_proba = train_catboost(
        X_train_proc, y_train, X_test_proc, y_test, scale_pos_weight
    )
    res_cb = evaluate_probs(y_test, cb_proba, threshold=0.5)
    results["CatBoost (base)"] = res_cb
    fpr, tpr, _ = roc_curve(y_test, cb_proba)
    roc_data["CatBoost (base)"] = (fpr, tpr)

    # SAINT-B
    cat_cardinalities = []
    if categorical_cols:
        for col in categorical_cols:
            card = int(X_train_proc[col].nunique())
            cat_cardinalities.append(card)
    num_features = len(numeric_cols)

    saint_model, saint_proba = train_saint(
        cat_train, num_train, y_train,
        cat_val, num_val, y_val,
        cat_test, num_test, y_test,
        cat_cardinalities, num_features,
        device=device
    )
    res_saint = evaluate_probs(y_test, saint_proba, threshold=0.5)
    results["SAINT-B (base)"] = res_saint
    fpr, tpr, _ = roc_curve(y_test, saint_proba)
    roc_data["SAINT-B (base)"] = (fpr, tpr)

    # Results table
    rows = []
    for name, res in results.items():
        rows.append({
            "Model": name,
            "Threshold": 0.5,
            "Accuracy": res["accuracy"],
            "Precision": res["precision"],
            "Recall": res["recall"],
            "F1 Score": res["f1"],
            "AUC": res["auc"],
            "Specificity": res["specificity"]
        })
    results_df = pd.DataFrame(rows)
    print("\n==============================")
    print("FINAL RESULTS TABLE (TEST)")
    print("==============================")
    print(results_df)

    csv_path = os.path.join(args.output_dir, "results_test.csv")
    results_df.to_csv(csv_path, index=False)
    print(f"\nSaved results table to: {csv_path}")

    # Precision-Recall scatter
    plt.figure(figsize=(6, 4))
    for name, res in results.items():
        plt.scatter(
            res["precision"],
            res["recall"],
            label=name
        )
    plt.xlabel("Precision")
    plt.ylabel("Recall")
    plt.title("Precision vs Recall (Test)")
    plt.legend()
    plt.tight_layout()
    pr_path = os.path.join(args.output_dir, "precision_recall_scatter.png")
    plt.savefig(pr_path, dpi=300)
    plt.close()
    print(f"Saved precision-recall scatter to: {pr_path}")

    # ROC curves
    plt.figure(figsize=(6, 5))
    for name, (fpr, tpr) in roc_data.items():
        auc_val = results[name]["auc"]
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc_val:.3f})")
    plt.plot([0, 1], [0, 1], "--", color="gray")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves (Test)")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.tight_layout()
    roc_path = os.path.join(args.output_dir, "roc_curves.png")
    plt.savefig(roc_path, dpi=300)
    plt.close()
    print(f"Saved ROC curves to: {roc_path}")


if __name__ == "__main__":
    main()
